{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_page(url, season):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to fetch data: HTTP {r.status_code}\")\n",
    "            return None, False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"table table-striped table-sortable player-stats highlight-stats season\")\n",
    "    if not table:\n",
    "        return None, False\n",
    "\n",
    "    headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    for row in rows:\n",
    "        data = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "        if data:\n",
    "            df.loc[len(df)] = data\n",
    "\n",
    "    # Extract and append player URLs\n",
    "    for span in table.find_all(\"span\", class_=\"txt-blue\"):\n",
    "        link = span.find('a')\n",
    "        if link:\n",
    "            url = link.get(\"href\")\n",
    "            name = link.text.strip()\n",
    "            df.loc[df.Player == name, \"Player_URL\"] = url\n",
    "\n",
    "    df['Year'] = season\n",
    "\n",
    "    # Pagination check\n",
    "    pagination = soup.find(\"div\", class_=\"table-pagination\")\n",
    "    next_page_link = pagination.find(\"a\", text=\"Next page\") if pagination else None\n",
    "    has_next_page = bool(next_page_link)\n",
    "\n",
    "    return df, has_next_page\n",
    "\n",
    "def scrape_seasons_early(base_url_template, start_year, end_year):\n",
    "    dfs = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{year + 1}\"\n",
    "        base_url = base_url_template.format(season=season)\n",
    "        print(f\"Scraping season: {season}\")\n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{base_url}?page={page}\"\n",
    "            df, has_next_page = scrape_page(url, season)\n",
    "            if df is None:\n",
    "                print(\"No more data or repeating page.\")\n",
    "                break\n",
    "            dfs.append(df)\n",
    "            if not has_next_page:\n",
    "                break\n",
    "            page += 1\n",
    "            time.sleep(random.uniform(3, 7))  # Randomized delay\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if dfs:\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        df_final.to_csv(\"nhl_player_stats_1917_to_1991.csv\", index=False)\n",
    "        return df_final\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "base_url_template = 'https://www.eliteprospects.com/league/nhl/stats/{season}'\n",
    "df_early = scrape_seasons_early(base_url_template, 1917, 1990)\n",
    "print(df_early)\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
